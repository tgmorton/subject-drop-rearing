# =========================================================================================
# ANNOTATED CONFIGURATION TEMPLATE
#
# This file serves as the master template and README for all experiment configurations.
# It explains how each section of the YAML file maps to a specific part of the
# `model_foundry` codebase, allowing you to design reproducible experiments.
#
# WORKFLOW:
# 1. Copy this file to `experiment_X_my_experiment.yaml`.
# 2. Modify the parameters below to define your experiment.
# 3. Run the preprocessing pipeline: `python -m model_foundry.cli preprocess configs/experiment_X...`
# 4. Run the training: `python -m model_foundry.cli run configs/experiment_X...`
# =========================================================================================

# A unique name for this experiment. Used for naming output directories and for logging.
experiment_name: "unique_experiment_name"

# --- Data Configuration ---
# Controls all aspects of data loading and batching.
# ACTIVATES: `model_foundry/data.py`
data:
  # The absolute starting point: your raw, untouched corpus.
  source_corpus: "data/raw/babylm_90M"

  # The final corpus file that will be fed to the model for training.
  # If you have a `dataset_manipulation` pipeline, this should be the `output_path`
  # of the VERY LAST step in that pipeline. If there are no manipulations,
  # this will be the same as `source_corpus`.
  training_corpus: "data/processed/final_ablated_corpus"

  # Passed to the PyTorch DataLoader.
  batch_size: 256
  max_sequence_length: 128

# --- Dataset Manipulation Pipeline ---
# Defines a sequence of preprocessing steps to perform on the `source_corpus`.
# ACTIVATES: `model_foundry/cli.py` (the `preprocess` command) which then calls
#            the specified scripts within the `preprocessing/` directory.
dataset_manipulation:
  # Each item in this list is a sequential step. The `type` key MUST match
  # the name of a Python script in `preprocessing/` (e.g., `remove_expletives.py`).

  - type: remove_expletives
    # The `input_path` for the first step is always the `source_corpus`.
    input_path: "data/raw/babylm_100M.txt"
    # The output of this step.
    output_path: "data/processed/intermediate_step1.txt"

  - type: impoverish_determiners
    # For subsequent steps, the `input_path` should be the `output_path` of the previous step.
    input_path: "data/processed/intermediate_step1.txt"
    output_path: "data/processed/final_ablated_corpus.txt"
    # Some scripts may take additional parameters, which you can define here.
    # These are passed as command-line arguments to the script.
    # parameters:
    #   replacement_token: "[MASK]"

# --- Tokenizer Configuration ---
# Defines how the vocabulary for this experiment is created.
# ACTIVATES: `model_foundry/tokenizer/train_tokenizer.py`
tokenizer:
  # The directory where the trained SentencePiece model (`.model`) and vocab (`.vocab`) will be saved.
  # Each experiment gets its own tokenizer directory.
  output_dir: "tokenizers/unique_experiment_name/"
  vocab_size: 50004

# --- Model Architecture ---
# Specifies the hyperparameters for the GPT-2 model.
# ACTIVATES: `model_foundry/model.py` which constructs the model object.
model:
  layers: 12
  embedding_size: 768
  hidden_size: 768
  intermediate_hidden_size: 3072
  attention_heads: 12
  activation_function: "GELU"
  dropout: 0.1
  attention_dropout: 0.1

# --- Training Procedure ---
# Controls the entire training loop, including optimization and checkpointing.
# ACTIVATES: `model_foundry/trainer.py`
training:
  # The directory where model checkpoints will be saved.
  output_dir: "models/unique_experiment_name/"

  # Optimizer hyperparameters.
  learning_rate: 0.0001
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-6

  # Learning rate scheduler parameters.
  warmup_steps: 10000

  # Total training duration.
  train_steps: 1000000
  epochs: 20

  # As specified in the proposal, this custom strategy saves checkpoints
  # at log-increasing steps (1, 2, 4, 8...) during the first epoch.
  checkpointing_strategy: "log_steps_first_epoch"
  
  # Checkpoint generation parameters (NEW)
  auto_generate_checkpoints: false  # Set to true for automatic schedule generation
  
  # First epoch configuration
  first_epoch_checkpoints: 20      # Number of checkpoints in first epoch
  
  # Subsequent epochs configuration
  subsequent_epochs_spacing: "log"  # "linear" or "log"
  log_base: 2                      # Base for logarithmic spacing (default 2)
  linear_interval: null             # Steps between checkpoints for linear spacing (null = auto)
  min_checkpoint_interval: 100     # Minimum steps between checkpoints

# --- Logging & Reproducibility ---
# Manages external logging and ensures the experiment can be perfectly replicated.
# ACTIVATES: `wandb` integration within `model_foundry/trainer.py`
logging:
  # Set to `true` to log metrics (loss, learning rate, etc.) to Weights & Biases.
  use_wandb: true
  wandb_project: "just-drop-the-subject" # The project name on your W&B account.

# This seed is used for all random initializations (model weights, data shuffling, etc.)
# to ensure that the only difference between experiments is the configuration itself.
random_seed: 42