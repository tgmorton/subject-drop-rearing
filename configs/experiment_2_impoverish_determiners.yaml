# ===================================================================
# EXPERIMENT 2: No Expletives + Poor Determiner Morphology
# GOAL: Test the combined effect of removing expletives and impoverishing
# [cite_start]the determiner system, relevant to Duguine's theory. [cite: 171, 174]
# ===================================================================

experiment_name: "exp2_no_expletives_poor_determiners"

# --- Data Configuration ---
data:
  source_corpus: "data/raw/train_90M/"
  # The final training corpus is the output of the final manipulation step.
  training_corpus: "data/processed/exp2_no_expletives_poor_determiners/"
  batch_size: 256
  max_sequence_length: 128

# --- Dataset Manipulation Pipeline ---
# The pipeline now has two sequential steps.
dataset_manipulation:
  - type: impoverish_determiners # This step runs second.
    # The input is the output from the previous step.
    input_path: "data/processed/exp1_remove_expletives/"
    output_path: "data/processed/exp2_impoverish_determiners/"
    parameters:
      chunk_size: 1000
      replacement_pool_dir: "data/processed/exp1_remove_expletives/replacement_pool_remainder/"
      skip_validation: true
      verbose: true

# --- Tokenizer Configuration ---
tokenizer:
  output_dir: "tokenizers/exp2_no_expletives_poor_determiners/"
  vocab_size: 50004

# --- Model Architecture ---
# Identical to the baseline.
model:
  layers: 12
  embedding_size: 768
  hidden_size: 768
  intermediate_hidden_size: 3072
  attention_heads: 12
  activation_function: "GELU"
  dropout: 0.1
  attention_dropout: 0.1

# --- Training Procedure ---
# Identical to the baseline.
training:
  output_dir: "models/exp2_no_expletives_poor_determiners/"
  learning_rate: 0.0001
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-6
  warmup_steps: 10000
  train_steps: 1000000
  checkpointing_strategy: "log_steps_first_epoch"
  epochs: 20

# --- Logging & Reproducibility ---
logging:
  use_wandb: true
  wandb_project: "just-drop-the-subject"
random_seed: 42