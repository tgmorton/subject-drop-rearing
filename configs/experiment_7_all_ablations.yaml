# ===================================================================
# EXPERIMENT 7: All Ablations (Most Impoverished)
# GOAL: Test how applying all ablations affects subject-drop learning
# ===================================================================

experiment_name: "experiment_7_all_ablations"

# --- Data Configuration ---
data:
  source_corpus: "data/raw/train_90M/"
  training_corpus: "data/processed/experiment_7_all_ablations/"
  batch_size: 256
  max_sequence_length: 128

# --- Dataset Manipulation Pipeline ---
dataset_manipulation:
  - remove_expletives
  - impoverish_determiners
  - remove_articles
  - lemmatize_verbs
  - remove_subject_pronominals

# --- Tokenizer Configuration ---
tokenizer:
  output_dir: "tokenizers/experiment_7_all_ablations/"
  vocab_size: 50004

# --- Model Architecture ---
model:
  layers: 12
  embedding_size: 768
  hidden_size: 768
  intermediate_hidden_size: 3072
  attention_heads: 12
  activation_function: "GELU"
  dropout: 0.1
  attention_dropout: 0.1

# --- Training Procedure ---
training:
  output_dir: "models/experiment_7_all_ablations/"
  learning_rate: 0.0001
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-6
  warmup_steps: 10000
  train_steps: 1000000
  epochs: 20
  
  # Checkpoint generation parameters
  auto_generate_checkpoints: true
  first_epoch_checkpoints: 20
  subsequent_epochs_spacing: "log"
  log_base: 2
  linear_interval: null
  min_checkpoint_interval: 100

# --- Logging & Reproducibility ---
logging:
  use_wandb: true
  wandb_project: "subject-drop-rearing"

random_seed: 42 