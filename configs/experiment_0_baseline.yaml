# ===================================================================
# EXPERIMENT 0: Baseline
# GOAL: Train the base model on the unaltered BabyLM corpus.
# This serves as the control against which all ablations are measured.
# ===================================================================

experiment_name: "exp0_baseline_model"

# --- Data Configuration ---
data:
  # Path to the raw, unprocessed training data.
  source_corpus: "data/raw/babylm_100M.txt"
  # The final training corpus is the same as the source in this case.
  training_corpus: "data/raw/babylm_100M.txt"
  # Configuration for the dataset loader.
  batch_size: 256
  max_sequence_length: 128

# --- Dataset Manipulation Pipeline ---
# This section is empty because this is the baseline experiment.
# No ablations are performed.
dataset_manipulation: []

# --- Tokenizer Configuration ---
tokenizer:
  # Where to save the trained tokenizer for this experiment.
  output_dir: "tokenizers/exp0_baseline/"
  [cite_start]vocab_size: 50004 # [cite: 207]

# --- Model Architecture ---
# Parameters for the GPT-2 model, based on your proposal's Table 5.
model:
  [cite_start]layers: 12 # [cite: 207]
  [cite_start]embedding_size: 768 # [cite: 207]
  [cite_start]hidden_size: 768 # [cite: 207]
  [cite_start]intermediate_hidden_size: 3072 # [cite: 207]
  [cite_start]attention_heads: 12 # [cite: 207]
  [cite_start]activation_function: "GELU" # [cite: 207]
  [cite_start]dropout: 0.1 # [cite: 207]
  [cite_start]attention_dropout: 0.1 # [cite: 207]

# --- Training Procedure ---
training:
  # Path to save model checkpoints.
  output_dir: "models/exp0_baseline/"
  # Training parameters from Table 5.
  [cite_start]learning_rate: 0.0001 # [cite: 207]
  [cite_start]adam_beta1: 0.9 # [cite: 207]
  [cite_start]adam_beta2: 0.999 # [cite: 207]
  [cite_start]adam_epsilon: 1.0e-6 # [cite: 207]
  [cite_start]warmup_steps: 10000 # [cite: 207]
  [cite_start]train_steps: 1000000 # [cite: 207]
  # Custom checkpointing strategy from the proposal.
  [cite_start]checkpointing_strategy: "log_steps_first_epoch" # [cite: 44]
  [cite_start]epochs: 20 # [cite: 48]

# --- Logging & Reproducibility ---
logging:
  # Integrate with Weights & Biases as per the proposal.
  [cite_start]use_wandb: true # [cite: 57]
  wandb_project: "just-drop-the-subject"
# A random seed for ensuring reproducibility between experiments.
random_seed: 42